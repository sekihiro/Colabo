{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sekihiro/Colabo/blob/master/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUS0ES-lfiQj",
        "colab_type": "text"
      },
      "source": [
        "## PyTorchを使ってLSTMで文章分類\n",
        "- 対象：livedoorニュースのタイトル文\n",
        "- https://qiita.com/m__k/items/841950a57a0d7ff05506\n",
        "- https://qiita.com/m__k/items/db1a81bb06607d5b0ec5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RPFPKsfnEZ",
        "colab_type": "code",
        "outputId": "7e3f5ca8-67fc-4c8a-d97e-902b16ef4cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbZgevpYluqN",
        "colab_type": "text"
      },
      "source": [
        "### livedoor ニュースコーパス 取得"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJCE4MBugy3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \"/content/drive/My Drive/git/LDCC/data\"\n",
        "%ls -al\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar zxvf ldcc-20140209.tar.gz\n",
        "%ls -al"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZp13Ubja-UN",
        "colab_type": "code",
        "outputId": "c3de6cf1-7f07-4dbc-81a1-7e20da8cb72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/git/LDCC/data/text/\"\n",
        "%ls -al"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/git/LDCC/data/text\n",
            "total 36\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[0m\u001b[01;34mdokujo-tsushin\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34mit-life-hack\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34mkaden-channel\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34mlivedoor-homme\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34mmovie-enter\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34mpeachy\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34msmax\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34msports-watch\u001b[0m/\n",
            "drwx------ 2 root root 4096 Feb  9  2014 \u001b[01;34mtopic-news\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EbduvF5bQzn",
        "colab_type": "code",
        "outputId": "4c5c9016-404f-482c-d876-38f13cb7330b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 11 07:22:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NPa02lBbStd",
        "colab_type": "code",
        "outputId": "711c30c5-fc44-4d70-8994-11025f0c4bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!nvcc -V"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooDElTJJbdeX",
        "colab_type": "code",
        "outputId": "65ac14b8-05ea-4502-be33-46e1a018aa06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#define CUDNN_MAJOR 7\n",
            "#define CUDNN_MINOR 6\n",
            "#define CUDNN_PATCHLEVEL 5\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "#include \"driver_types.h\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h99S_qY-lz_p",
        "colab_type": "text"
      },
      "source": [
        "### **MeCab** を Google-Colaboratory で使えるようにする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pU0SisclrQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CieEjoH_hegL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import easydict\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pprint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import cloudpickle\n",
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import linecache\n",
        "import MeCab\n",
        "import re\n",
        "import collections\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpNFRNqnyWM5",
        "colab_type": "code",
        "outputId": "d1b71fc3-128c-4277-f259-823d61a808b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 再現性確保\n",
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    return seed\n",
        "\n",
        "# 再現性確保\n",
        "ret = seed_everything(1234)\n",
        "print('seed : {}'.format(ret))\n",
        "\n",
        "# select device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device : {}'.format(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed : 1234\n",
            "device : cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvG8C99GmzVd",
        "colab_type": "text"
      },
      "source": [
        "### データ準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CinTvrrdpi2F",
        "colab_type": "code",
        "outputId": "a9b8bf46-25bd-4882-fb09-4256b1fd18ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# テキスト内容を確認１\n",
        "!head \"/content/drive/My Drive/git/LDCC/data/text/dokujo-tsushin/dokujo-tsushin-4778030.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://news.livedoor.com/article/detail/4778030/\n",
            "2010-05-22T14:30:00+0900\n",
            "友人代表のスピーチ、独女はどうこなしている？\n",
            "　もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……という「お祝い貧乏」状態の人も多いのではないだろうか？　さらに出席回数を重ねていくと、こんなお願いごとをされることも少なくない。\n",
            "\n",
            "　「お願いがあるんだけど……友人代表のスピーチ、やってくれないかな？」\n",
            "\n",
            "　さてそんなとき、独女はどう対応したらいいか？\n",
            "\n",
            "　最近だとインターネット等で検索すれば友人代表スピーチ用の例文サイトがたくさん出てくるので、それらを参考にすれば、無難なものは誰でも作成できる。しかし由利さん（33歳）はネットを参考にして作成したものの「これで本当にいいのか不安でした。一人暮らしなので聞かせて感想をいってくれる人もいないし、かといって他の友人にわざわざ聞かせるのもどうかと思うし……」ということで活用したのが、なんとインターネットの悩み相談サイトに。そこに作成したスピーチ文を掲載し「これで大丈夫か添削してください」とメッセージを送ったというのである。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkByQB1JouDB",
        "colab_type": "code",
        "outputId": "dba19760-03f5-4308-ac25-7aa76599b77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# テキスト内容を確認２\n",
        "!head \"/content/drive/My Drive/git/LDCC/data/text/it-life-hack/it-life-hack-6292880.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://news.livedoor.com/article/detail/6292880/\n",
            "2012-02-19T13:00:00+0900\n",
            "旧式Macで禁断のパワーアップ！最新PCやソフトを一挙にチェック【ITフラッシュバック】\n",
            "テレビやTwitterと連携できるパソコンや、プロセッサや切り替わるパソコンなど、面白いパソコンが次から次へと登場した。旧式Macの禁断ともいえるパワーアップ方法から、NECの最新PC、話題のThinkPad X1 Hybrid、新セキュリティソフトまで一挙に紹介しよう。\n",
            "\n",
            "■インテル SSD 520をMacに装着！旧式Macはどれほど高速化するのか (上)\n",
            "インテルが最新SSD「520シリーズ」を発売した。現行SSDの中でもトップクラスの性能を誇る同製品を、旧型Macの高速化を図るというポイントでレビューしてみた。少し風変わりなレビューとなるが、どの程度の効果があるか、期待大である。\n",
            "\n",
            "\n",
            "■http://itlifehack.jp/archives/6716997.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPZXctFEgS4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH = \"/content/drive/My Drive/git/LDCC/\"\n",
        "DATA_PATH = BASE_PATH + 'data/text/'\n",
        "PICKLE_PATH = BASE_PATH + 'data/ldcc.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSUQrjrghjqq",
        "colab_type": "code",
        "outputId": "52083796-4381-4e16-8dc6-331c7b66612b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# LDCCテキストファイルからタイトル行をDateFrameに格納し、pickle形式で保存\n",
        "\n",
        "# カテゴリを配列で取得\n",
        "categories = [name for name in os.listdir(DATA_PATH) if os.path.isdir(DATA_PATH + name)]\n",
        "print(categories)\n",
        "time.sleep(1)\n",
        "# ['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']\n",
        "\n",
        "if os.path.isfile(PICKLE_PATH):\n",
        "    print(PICKLE_PATH + ' is exists ... ')\n",
        "else:\n",
        "    # データフレーム作成\n",
        "    datasets = pd.DataFrame(columns=[\"title\", \"category\"])\n",
        "    for cat in categories:\n",
        "        path = DATA_PATH + cat + \"/*.txt\"\n",
        "        files = glob(path)\n",
        "        print('\\n' + cat + '\\n')\n",
        "        time.sleep(1)\n",
        "        for text_name in tqdm(files):\n",
        "            # 各テキストファイルの3行目にタイトル文字列の記載がある\n",
        "            title = linecache.getline(text_name, 3)\n",
        "            s = pd.Series([title, cat], index=datasets.columns)\n",
        "            datasets = datasets.append(s, ignore_index=True)\n",
        "\n",
        "    # データフレームシャッフル\n",
        "    print('\\n len : {}'.format(len(datasets)))\n",
        "    datasets = datasets.sample(frac=1).reset_index(drop=True)\n",
        "    datasets.head()\n",
        "\n",
        "    # PICKLE形式で保存\n",
        "    datasets.to_pickle(PICKLE_PATH)\n",
        "    print(PICKLE_PATH + ' is saved ...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dokujo-tsushin', 'livedoor-homme', 'kaden-channel', 'smax', 'topic-news', 'peachy', 'movie-enter', 'it-life-hack', 'sports-watch']\n",
            "/content/drive/My Drive/git/LDCC/data/ldcc.pkl is exists ... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rMMr1lh4XHp",
        "colab_type": "code",
        "outputId": "56ecab38-7623-406b-c6e2-dcbd2dd2fcc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# PICKLE形式のDataFrameを読み込む\n",
        "datasets = pd.read_pickle(PICKLE_PATH)\n",
        "datasets.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>貞子がついに上戸彩と並んだ！ 野球カード化が決定\\n</td>\n",
              "      <td>movie-enter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>人間のボクシングは死に絶え、時代は“最強”ロボット格闘技へ\\n</td>\n",
              "      <td>movie-enter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>乗り換えたいけど、スマホって本当に便利？　ケータイ大好きOLの女子座談会へ潜入\\n</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>くさったよめがあらわれた！vol.05「自分を変えたい…それは楽しめることをみつけること!!...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ウリナリ的な挑戦企画が人気か？　日テレ「ヒルナンデス！」が視聴率初の昼トップに【話題】\\n</td>\n",
              "      <td>kaden-channel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title        category\n",
              "0                         貞子がついに上戸彩と並んだ！ 野球カード化が決定\\n     movie-enter\n",
              "1                    人間のボクシングは死に絶え、時代は“最強”ロボット格闘技へ\\n     movie-enter\n",
              "2          乗り換えたいけど、スマホって本当に便利？　ケータイ大好きOLの女子座談会へ潜入\\n  livedoor-homme\n",
              "3  くさったよめがあらわれた！vol.05「自分を変えたい…それは楽しめることをみつけること!!...  dokujo-tsushin\n",
              "4      ウリナリ的な挑戦企画が人気か？　日テレ「ヒルナンデス！」が視聴率初の昼トップに【話題】\\n   kaden-channel"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-WdP8bm4p7",
        "colab_type": "text"
      },
      "source": [
        "### データ前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbvNUcjirjQQ",
        "colab_type": "text"
      },
      "source": [
        "PyTorchでLSTMをする際、食わせるインプットデータは３次元のテンソルある必要があります。<br>具体的には、文章の長さ × バッチサイズ × ベクトル次元数 となっています。<br>\n",
        "今回のインプットデータは文章（livedoorニュースのタイトル文）であり、この文章を3次元テンソルに変換する必要があります。<br>\n",
        "バッチサイズは一旦無視して、ひとまず文章を以下のように２次元のマトリクスに変換することを考えます。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "人工知能は人間の仕事を奪った\n",
        "\n",
        "(形態素解析) → ['人工','知能','は','人間','の','仕事','を','奪っ','た']\n",
        "\n",
        "(各単語をベクトルで置換)→<br>\n",
        "[<br>\n",
        "[0.2 0.5 -0.9 1.3 ...], # 「人口」の単語ベクトル<br>\n",
        "[1.3 0.1 2.9 -1.3 ...], # 「知能」の単語ベクトル<br>\n",
        "...<br>\n",
        "[0.9 -0.3 -0.1 3.0 ...] # 「た」の単語ベクトル<br>\n",
        "]<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SwffeYktLh9",
        "colab_type": "text"
      },
      "source": [
        "単語のベクトルは例えばWord2Vecで学習済みのものがあればそれを使う方が精度が良いらしいですが、<br>一旦はPyTorchの torch.nn.Embedding を使いましょう。<br>\n",
        "こいつの詳細はPyTorchのチュートリアルに任せますが、要はランダムな単語ベクトル群を生成してくれるやつです。<br>実際に使ってみると分かりやすいです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwdJUIF5f30S",
        "colab_type": "code",
        "outputId": "5d97362e-9b8c-4f1e-f708-91a11d0c6f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# torch.nn.Embedding の使い方\n",
        "\n",
        "# 以下の宣言で行が単語ベクトル、列が単語のインデックスのマトリクスを生成してる感じ\n",
        "embeds = nn.Embedding(10, 6) # (Embedding(単語の合計数, ベクトル次元数))\n",
        "\n",
        "# ３行目の要素を取り出したいならば\n",
        "w1 = torch.tensor([2])\n",
        "print(embeds(w1))\n",
        "# tensor([[-1.5947, -0.8387,  0.7669, -0.9644, -0.7902,  2.7167]],\n",
        "#        grad_fn=<EmbeddingBackward>)\n",
        "\n",
        "# 3行目、5行目、１０行目の要素を取り出したいならば、\n",
        "w2 = torch.tensor([2,4,9])\n",
        "print(embeds(w2))\n",
        "# tensor([[-1.5947, -0.8387,  0.7669, -0.9644, -0.7902,  2.7167],\n",
        "#        [ 0.0405,  1.4236,  0.1947,  0.2609,  0.2047, -1.4964],\n",
        "#        [ 1.7325, -0.2543, -0.5139, -0.9527, -0.1344,  0.0984]],\n",
        "#       grad_fn=<EmbeddingBackward>)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.3424, -1.4020,  0.3206, -1.0219,  0.7988, -0.0923]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "tensor([[-0.3424, -1.4020,  0.3206, -1.0219,  0.7988, -0.0923],\n",
            "        [-0.1706, -1.4594,  0.2207,  0.2463, -1.3248,  0.6970],\n",
            "        [ 1.0534,  0.3692,  0.0628, -0.3297, -1.7970,  0.8728]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHyuD0H4tgPk",
        "colab_type": "code",
        "outputId": "cfcbd31a-cded-4ab1-cd5b-4082651ab083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        }
      },
      "source": [
        "# 文章を単語IDの系列データとして変換する方法の確認\n",
        "\n",
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "def make_wakati(sentence):\n",
        "    # MeCabで分かち書き\n",
        "    sentence = tagger.parse(sentence)\n",
        "    # 半角全角英数字除去\n",
        "    sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
        "    # 記号もろもろ除去\n",
        "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
        "    # スペースで区切って形態素の配列へ\n",
        "    wakati = sentence.split(\" \")\n",
        "    # 空の要素は削除\n",
        "    wakati = list(filter((\"\").__ne__, wakati))\n",
        "    return wakati\n",
        "\n",
        "# テスト\n",
        "test = \"今日はそういう日だったのか！Googleロゴが変わっている理由\"\n",
        "print('make_wakati TEST:')\n",
        "print(make_wakati(test))\n",
        "\n",
        "# 単語ID辞書を作成する\n",
        "word2index = {}\n",
        "for title in datasets[\"title\"]:\n",
        "    wakati = make_wakati(title)\n",
        "    for word in wakati:\n",
        "        if word in word2index: continue\n",
        "        word2index[word] = len(word2index)\n",
        "print(\"\\nvocab size : {}\\n\".format(len(word2index)))\n",
        "# (例) vocab size :  13229\n",
        "\n",
        "# どんな辞書になっているか先頭から20件出力\n",
        "i = 0\n",
        "print('key\\tval\\n----------')\n",
        "for mykey, myvalue in word2index.items():\n",
        "    i = i + 1\n",
        "    if i<= 10 or (i >=470 and i<=480) or (i >= 9900 and i<= 9910):\n",
        "        print(\"{}\\t{}\".format(mykey, myvalue))\n",
        "\n",
        "# 文章を単語IDの系列データに変換\n",
        "# PyTorchのLSTMのインプットになるデータなので、もちろんtensor型で\n",
        "def sentence2index(sentence):\n",
        "    wakati = make_wakati(sentence)\n",
        "    return torch.tensor([word2index[w] for w in wakati], dtype=torch.long)\n",
        "\n",
        "# テスト\n",
        "# 実際にLDCCに存在する「title」にすること\n",
        "#test = \"これで今日からExcelの達人！ビジネスに必須のExcel基本ワザ【知っ得！虎の巻】\"\n",
        "print('\\nsentence2index TEST:')\n",
        "print(test + '\\n↓')\n",
        "ret = sentence2index(test)\n",
        "print(type(ret))\n",
        "print(ret)\n",
        "# (例) tensor([11320,     3,   449,  5483,    26,  3096,  1493,  1368,     3, 11371, 7835,   174,  8280])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_wakati TEST:\n",
            "['今日', 'は', 'そういう', '日', 'だっ', 'た', 'の', 'か', 'ロゴ', 'が', '変わっ', 'て', 'いる', '理由']\n",
            "\n",
            "vocab size : 9965\n",
            "\n",
            "key\tval\n",
            "----------\n",
            "貞子\t0\n",
            "が\t1\n",
            "ついに\t2\n",
            "上戸\t3\n",
            "彩\t4\n",
            "と\t5\n",
            "並ん\t6\n",
            "だ\t7\n",
            "野球\t8\n",
            "カード\t9\n",
            "森\t469\n",
            "監督\t470\n",
            "作品\t471\n",
            "今日\t472\n",
            "秋刀魚\t473\n",
            "ビデオ\t474\n",
            "次世代\t475\n",
            "ジェットコースターショッカーミステリー\t476\n",
            "スクリーム\t477\n",
            "ブルー\t478\n",
            "レイ\t479\n",
            "エチカ\t9899\n",
            "ぐっすり\t9900\n",
            "眠れる\t9901\n",
            "ヒーローズ\t9902\n",
            "発狂\t9903\n",
            "悲し\t9904\n",
            "げ\t9905\n",
            "コンパス\t9906\n",
            "客観\t9907\n",
            "進め\t9908\n",
            "ダイヤ\t9909\n",
            "\n",
            "sentence2index TEST:\n",
            "今日はそういう日だったのか！Googleロゴが変わっている理由\n",
            "↓\n",
            "<class 'torch.Tensor'>\n",
            "tensor([ 472,   15, 1783,  285, 1207,   37,   13,   56, 1012,    1,  689,  113,\n",
            "         249,  802])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbUnxxUdcLID",
        "colab_type": "code",
        "outputId": "27fa8e7d-6c62-40a7-fc50-d71916d87817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "# 全単語数を取得\n",
        "# word2index：\n",
        "# 　全てデータ(LDCCのニュースタイトル)をMeCabで形態素に分かち書きし、\n",
        "# 　形態素毎にINDEXを付与したもの\n",
        "VOCAB_SIZE = len(word2index)\n",
        "print('VOCAB_SIZE : {}\\n'.format(VOCAB_SIZE))\n",
        "\n",
        "# 単語のベクトル数を定義\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# 単語IDの系列データに変換\n",
        "# test変数の内容を形態素に分けて、それぞれの形態素のINDEX(word2indexを使う)を得る\n",
        "# 実際にLDCCに存在する「title」にすること\n",
        "test = \"今日はそういう日だったのか！Googleロゴが変わっている理由\"\n",
        "inputs = sentence2index(test)\n",
        "print(test + '\\n↓')\n",
        "print('inputs : {}\\n'.format(inputs))\n",
        "test2 = \"これで今日からExcelの達人！ビジネスに必須のExcel基本ワザ【知っ得！虎の巻】\"\n",
        "inputs2 = sentence2index(test2)\n",
        "print(test2 + '\\n↓')\n",
        "print('inputs : {}\\n'.format(inputs2))\n",
        "\n",
        "# 各単語のベクトルをまとめて取得\n",
        "# 各形態素毎に10次元のベクトルを得る\n",
        "# ２次元のマトリクスとする\n",
        "embeds = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM) # Embedding(単語の合計数, ベクトル次元数)\n",
        "sentence_matrix = embeds(inputs)\n",
        "print(test + '\\n↓')\n",
        "print(sentence_matrix.size())\n",
        "print(sentence_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB_SIZE : 9965\n",
            "\n",
            "今日はそういう日だったのか！Googleロゴが変わっている理由\n",
            "↓\n",
            "inputs : tensor([ 472,   15, 1783,  285, 1207,   37,   13,   56, 1012,    1,  689,  113,\n",
            "         249,  802])\n",
            "\n",
            "これで今日からExcelの達人！ビジネスに必須のExcel基本ワザ【知っ得！虎の巻】\n",
            "↓\n",
            "inputs : tensor([ 134,  105,  472,  176,   13, 7126,  124,   64, 2782,   13,  881,  123,\n",
            "         129,  130,  131])\n",
            "\n",
            "今日はそういう日だったのか！Googleロゴが変わっている理由\n",
            "↓\n",
            "torch.Size([14, 10])\n",
            "tensor([[ 1.8636e+00, -1.0910e+00, -1.1969e+00,  3.3377e-01, -4.4827e-01,\n",
            "         -3.6077e-01,  1.4155e+00, -5.0984e-01, -9.5917e-01,  7.4717e-01],\n",
            "        [-6.3918e-02,  3.0292e+00,  2.5377e-01,  8.2522e-02, -1.1314e+00,\n",
            "          3.9972e-01,  3.5863e-01, -2.3594e+00, -1.3954e+00,  8.6747e-01],\n",
            "        [ 6.9869e-02, -1.0371e+00,  3.8296e-01, -2.0071e+00,  1.9310e-01,\n",
            "         -4.5787e-02, -7.6356e-01,  1.1115e+00,  2.2504e-01, -1.8004e+00],\n",
            "        [-1.4746e-01, -6.4389e-01, -7.5567e-01,  1.1182e+00, -2.1282e+00,\n",
            "         -1.7754e+00,  3.3990e-01,  1.6427e+00, -1.8038e+00,  3.7238e-01],\n",
            "        [ 6.8435e-01, -2.4232e+00,  4.0293e-01, -1.3072e+00,  3.4986e-01,\n",
            "          5.8635e-01,  1.2099e+00, -2.1425e-03, -6.5326e-01, -5.9161e-01],\n",
            "        [-1.2708e+00,  2.9587e-01,  1.5027e-01,  1.1833e+00, -7.2822e-01,\n",
            "         -1.2537e+00, -7.3179e-02, -1.4961e-02,  1.4511e-01,  2.0197e-01],\n",
            "        [ 1.7114e+00,  1.4621e+00,  2.3290e-01, -1.1014e+00, -1.2473e+00,\n",
            "         -7.4845e-01, -1.8821e+00, -1.8542e+00,  4.0218e-01,  4.2378e-01],\n",
            "        [-6.7843e-01, -1.4126e+00, -6.1121e-01, -2.7585e-01, -4.9035e-01,\n",
            "          2.5709e-01, -2.9133e-01, -6.5935e-01, -6.7626e-01,  9.3721e-02],\n",
            "        [ 7.3691e-01,  1.6472e-01,  9.1276e-01, -1.0048e+00,  7.9981e-01,\n",
            "          4.5312e-01, -5.6621e-01, -4.5102e-01,  5.5302e-01, -3.2944e-01],\n",
            "        [-1.7399e+00,  1.2993e-01,  8.5187e-01, -1.4102e+00, -1.0710e-01,\n",
            "         -8.0181e-01, -2.6779e-01, -1.2537e-01, -1.5038e+00, -3.2867e-01],\n",
            "        [ 8.5264e-01, -6.0303e-01,  1.5271e+00,  2.0740e+00,  4.0698e-02,\n",
            "          5.8955e-01,  2.5090e-01, -2.4481e+00, -2.4741e-01,  1.3679e+00],\n",
            "        [-5.3245e-01, -7.0218e-02,  5.3435e-01, -1.2356e+00,  1.8639e+00,\n",
            "          2.0044e+00, -1.5266e-01,  7.9305e-01,  2.2392e+00, -1.0865e+00],\n",
            "        [ 1.3622e+00, -1.8142e+00,  1.0506e+00, -5.7725e-01,  2.1764e-01,\n",
            "         -7.1780e-01,  2.1859e-02, -1.0560e-01, -4.8707e-01, -1.7926e-01],\n",
            "        [ 5.8126e-01,  6.2402e-01, -9.2453e-01, -1.7001e+00,  1.6465e+00,\n",
            "          2.0355e+00,  2.7664e-01, -2.5354e+00, -7.2019e-01, -4.2933e-01]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pOvpb8goVFU",
        "colab_type": "text"
      },
      "source": [
        "PyTorchでLSTMをする際、食わせるインプットデータは３次元のテンソルある必要があります。<br>\n",
        "具体的には、文章の長さ × バッチサイズ × ベクトル次元数 となっています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgACMOpsgYs5",
        "colab_type": "code",
        "outputId": "6e273cf9-be53-493b-de99-fff116b59288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# ２次元 → ３次元\n",
        "# 今回は（とりあえず）バッチサイズは１とする\n",
        "# viewの引数の-1は、「残り全て」を示す\n",
        "# 第１引数：タイトルを分かち書きした形態素数\n",
        "# 第２引数：バッチサイズ\n",
        "# 第３引数：残り\n",
        "# バッチサイズ「1」の場合は、\n",
        "# 形態素数×ベクトル数と形態素数×バッチサイズ(1)×ベクトル数で\n",
        "# 形態素数とベクトル数は、２次元でも３次元でも変化しない\n",
        "sentence_matrix.view(len(sentence_matrix), 1, -1).size()\n",
        "#print(sentence_matrix3)\n",
        "#print(type(sentence_matrix3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 1, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl6NvOkcpWQP",
        "colab_type": "text"
      },
      "source": [
        "### モデル定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kegh8XbeoUG-",
        "colab_type": "code",
        "outputId": "aec3c4dd-f618-479f-dddd-a5246ca10b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# torch.nn.LSTM の使い方\n",
        "\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "s1 = \"今日はそういう日だったのか！Googleロゴが変わっている理由\"\n",
        "print(make_wakati(s1))\n",
        "\n",
        "VOCAB_SIZE = len(word2index)\n",
        "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
        "\n",
        "# 単語IDの系列データに変換\n",
        "inputs1 = sentence2index(s1)\n",
        "print('sentence2index : {}\\n'.format(inputs1))\n",
        "\n",
        "# 各形態素毎に10次元のベクトルを得る (2次元)\n",
        "embeds = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM) # Embedding(単語の合計数, ベクトル次元数)\n",
        "emb1 = embeds(inputs1)\n",
        "print(emb1.size())\n",
        "print('emb1 : {}\\n'.format(emb1))\n",
        "\n",
        "# 2次元 → 3次元\n",
        "lstm_inputs1 = emb1.view(len(inputs1), 1, -1)\n",
        "print(lstm_inputs1.size())\n",
        "print('lstm_inputs1 : {}\\n'.format(lstm_inputs1))\n",
        "\n",
        "# LSTMコンストラクタ\n",
        "# 引数(入力ベクトルの次元数, 隠れ状態の次元数)\n",
        "lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM)\n",
        "\n",
        "# LSTM予測\n",
        "# out1：出力\n",
        "# out2：[0]最後の隠れ層の状態, [1]最後のメモリセル\n",
        "out1, out2 = lstm(lstm_inputs1)\n",
        "print('out1 : {}\\n'.format(out1))\n",
        "print('out2[0] : {}\\n'.format(out2[0]))\n",
        "print('out2[1] : {}\\n'.format(out2[1]))\n",
        "\n",
        "# 今回の例で言えば、文章(many)を１つのカテゴリに分類(one)したいので、 \n",
        "# many to one のモデルです。なので、LSTMの最後の隠れ層の出力を使うことになるでしょう。\n",
        "print('----------------------------------------')\n",
        "print('必要な情報：')\n",
        "print('out2[0] : {}\\n'.format(out2[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['今日', 'は', 'そういう', '日', 'だっ', 'た', 'の', 'か', 'ロゴ', 'が', '変わっ', 'て', 'いる', '理由']\n",
            "VOCAB_SIZE : 9965\n",
            "sentence2index : tensor([ 472,   15, 1783,  285, 1207,   37,   13,   56, 1012,    1,  689,  113,\n",
            "         249,  802])\n",
            "\n",
            "torch.Size([14, 10])\n",
            "emb1 : tensor([[-0.7391, -0.2699,  0.5971, -0.1804, -1.0321, -0.2202,  1.0034, -0.8345,\n",
            "          1.1180, -0.1778],\n",
            "        [-1.3238,  1.4525, -0.6788,  0.5478,  0.2297, -0.6999, -0.4498,  0.0369,\n",
            "         -0.9946, -0.6238],\n",
            "        [-1.8401,  1.1266,  1.6470,  0.3206,  1.5518,  0.7403,  0.7466,  0.3278,\n",
            "         -0.3524, -1.9578],\n",
            "        [-0.3467,  0.7206,  0.2003, -0.9189, -0.8895, -1.4803,  2.0345, -0.5642,\n",
            "          0.5577,  1.0044],\n",
            "        [ 0.1208, -0.2749, -1.0806, -0.3476,  0.7676, -1.5509, -0.4261,  1.8126,\n",
            "          0.5257, -0.3452],\n",
            "        [ 1.8475, -0.4634,  1.7881, -1.4666,  0.3842,  0.3938, -0.5951,  0.1148,\n",
            "         -0.3893, -0.8532],\n",
            "        [-0.2698,  0.7935,  1.0303, -1.4394,  0.3163, -0.7022,  0.8305,  0.7043,\n",
            "         -1.6797,  0.2842],\n",
            "        [-1.2407,  0.4540,  0.4708, -0.9328, -0.7159, -1.1900,  0.5994,  1.1392,\n",
            "          0.8330,  0.9954],\n",
            "        [ 1.4003,  0.9999, -2.4542, -0.4991, -1.5987,  2.2171, -0.3251, -0.2005,\n",
            "          0.5892,  1.0451],\n",
            "        [ 1.3878,  1.1845,  0.7127, -2.0589,  0.8396, -0.8654, -0.5142,  1.9004,\n",
            "         -1.6144, -0.8174],\n",
            "        [ 2.9488, -1.7579,  1.0681, -0.2192, -1.1068,  2.6449,  0.5908,  0.0969,\n",
            "          0.6237, -1.0383],\n",
            "        [ 1.9105, -0.4809, -0.8382,  0.9300,  0.0922,  1.4196,  0.2923,  1.6863,\n",
            "         -1.0383, -1.7831],\n",
            "        [-0.3671,  1.1951,  0.9981, -0.0367, -0.3049,  1.8148, -1.0511, -1.0102,\n",
            "          0.0921,  0.0815],\n",
            "        [-0.0267, -0.6335,  0.9399,  2.5088, -0.7971,  0.6926, -0.6376, -1.8654,\n",
            "          0.4519,  0.3654]], grad_fn=<EmbeddingBackward>)\n",
            "\n",
            "torch.Size([14, 1, 10])\n",
            "lstm_inputs1 : tensor([[[-0.7391, -0.2699,  0.5971, -0.1804, -1.0321, -0.2202,  1.0034,\n",
            "          -0.8345,  1.1180, -0.1778]],\n",
            "\n",
            "        [[-1.3238,  1.4525, -0.6788,  0.5478,  0.2297, -0.6999, -0.4498,\n",
            "           0.0369, -0.9946, -0.6238]],\n",
            "\n",
            "        [[-1.8401,  1.1266,  1.6470,  0.3206,  1.5518,  0.7403,  0.7466,\n",
            "           0.3278, -0.3524, -1.9578]],\n",
            "\n",
            "        [[-0.3467,  0.7206,  0.2003, -0.9189, -0.8895, -1.4803,  2.0345,\n",
            "          -0.5642,  0.5577,  1.0044]],\n",
            "\n",
            "        [[ 0.1208, -0.2749, -1.0806, -0.3476,  0.7676, -1.5509, -0.4261,\n",
            "           1.8126,  0.5257, -0.3452]],\n",
            "\n",
            "        [[ 1.8475, -0.4634,  1.7881, -1.4666,  0.3842,  0.3938, -0.5951,\n",
            "           0.1148, -0.3893, -0.8532]],\n",
            "\n",
            "        [[-0.2698,  0.7935,  1.0303, -1.4394,  0.3163, -0.7022,  0.8305,\n",
            "           0.7043, -1.6797,  0.2842]],\n",
            "\n",
            "        [[-1.2407,  0.4540,  0.4708, -0.9328, -0.7159, -1.1900,  0.5994,\n",
            "           1.1392,  0.8330,  0.9954]],\n",
            "\n",
            "        [[ 1.4003,  0.9999, -2.4542, -0.4991, -1.5987,  2.2171, -0.3251,\n",
            "          -0.2005,  0.5892,  1.0451]],\n",
            "\n",
            "        [[ 1.3878,  1.1845,  0.7127, -2.0589,  0.8396, -0.8654, -0.5142,\n",
            "           1.9004, -1.6144, -0.8174]],\n",
            "\n",
            "        [[ 2.9488, -1.7579,  1.0681, -0.2192, -1.1068,  2.6449,  0.5908,\n",
            "           0.0969,  0.6237, -1.0383]],\n",
            "\n",
            "        [[ 1.9105, -0.4809, -0.8382,  0.9300,  0.0922,  1.4196,  0.2923,\n",
            "           1.6863, -1.0383, -1.7831]],\n",
            "\n",
            "        [[-0.3671,  1.1951,  0.9981, -0.0367, -0.3049,  1.8148, -1.0511,\n",
            "          -1.0102,  0.0921,  0.0815]],\n",
            "\n",
            "        [[-0.0267, -0.6335,  0.9399,  2.5088, -0.7971,  0.6926, -0.6376,\n",
            "          -1.8654,  0.4519,  0.3654]]], grad_fn=<ViewBackward>)\n",
            "\n",
            "out1 : tensor([[[-2.0256e-02, -5.0220e-03,  7.4164e-02,  ..., -3.5597e-02,\n",
            "          -4.8946e-02,  7.8755e-02]],\n",
            "\n",
            "        [[ 1.5338e-02,  4.5646e-02,  6.9265e-02,  ..., -5.6884e-02,\n",
            "          -2.7025e-02,  1.1734e-01]],\n",
            "\n",
            "        [[ 2.4989e-02,  3.3631e-02,  9.5391e-02,  ..., -2.3746e-03,\n",
            "          -6.9009e-02,  1.1281e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.1690e-02, -4.7931e-02, -1.2317e-01,  ...,  1.1001e-01,\n",
            "           1.0165e-04, -5.8246e-02]],\n",
            "\n",
            "        [[-3.7204e-02, -6.8952e-02, -2.3603e-02,  ...,  1.0321e-01,\n",
            "           1.2546e-02, -1.6147e-02]],\n",
            "\n",
            "        [[-3.3040e-02,  2.2914e-02,  6.0517e-02,  ..., -4.1805e-03,\n",
            "           5.1668e-02,  6.3663e-02]]], grad_fn=<StackBackward>)\n",
            "\n",
            "out2[0] : tensor([[[-0.0330,  0.0229,  0.0605, -0.0502,  0.1159, -0.0832,  0.0181,\n",
            "          -0.0414,  0.0152,  0.0162,  0.0039,  0.0205,  0.1224, -0.0678,\n",
            "          -0.0016,  0.0030,  0.0714,  0.0776,  0.0526,  0.0383,  0.0588,\n",
            "           0.0413, -0.0026, -0.0296,  0.0914, -0.0600,  0.0766, -0.0193,\n",
            "           0.1149,  0.0862, -0.0560,  0.1501,  0.0642,  0.0824, -0.1534,\n",
            "          -0.0205, -0.0094, -0.1246,  0.0842, -0.0427,  0.0448,  0.0559,\n",
            "          -0.0647, -0.0327,  0.0112,  0.0926, -0.0233,  0.1068,  0.0093,\n",
            "          -0.0467, -0.0242,  0.0183, -0.0426, -0.0768, -0.0253, -0.0574,\n",
            "           0.0012,  0.0931,  0.0293,  0.0580,  0.0271,  0.0222, -0.0519,\n",
            "          -0.0165,  0.0062, -0.0984, -0.0308, -0.0584, -0.0633, -0.0461,\n",
            "          -0.2023,  0.0276, -0.0247,  0.0491,  0.0284,  0.0441, -0.0409,\n",
            "           0.0645, -0.0595,  0.0590, -0.0676,  0.0894, -0.1205,  0.1154,\n",
            "          -0.0084,  0.0093, -0.0984, -0.0789,  0.0636,  0.0630,  0.0213,\n",
            "           0.0259, -0.0153, -0.0056,  0.0022,  0.0338,  0.0588,  0.0948,\n",
            "          -0.0637, -0.0548, -0.0830,  0.0116,  0.0889,  0.0932, -0.0381,\n",
            "           0.1328, -0.0079, -0.1360, -0.0378,  0.0491, -0.0194,  0.0115,\n",
            "           0.0412,  0.0495, -0.0495, -0.0644,  0.0318,  0.0804,  0.0448,\n",
            "           0.0333, -0.0192,  0.0558, -0.1449,  0.0208,  0.0227, -0.0042,\n",
            "           0.0517,  0.0637]]], grad_fn=<StackBackward>)\n",
            "\n",
            "out2[1] : tensor([[[-0.0851,  0.0434,  0.1114, -0.1021,  0.2281, -0.1715,  0.0389,\n",
            "          -0.0796,  0.0309,  0.0350,  0.0082,  0.0403,  0.2968, -0.1196,\n",
            "          -0.0032,  0.0062,  0.1407,  0.1368,  0.1148,  0.0804,  0.1046,\n",
            "           0.0869, -0.0048, -0.0535,  0.1998, -0.1099,  0.1591, -0.0430,\n",
            "           0.2530,  0.1551, -0.1291,  0.3164,  0.1526,  0.1764, -0.2852,\n",
            "          -0.0472, -0.0231, -0.3035,  0.1900, -0.0781,  0.0898,  0.1514,\n",
            "          -0.1464, -0.0621,  0.0251,  0.1768, -0.0499,  0.2312,  0.0196,\n",
            "          -0.0778, -0.0613,  0.0371, -0.0811, -0.1456, -0.0459, -0.1277,\n",
            "           0.0025,  0.1683,  0.0729,  0.1213,  0.0509,  0.0477, -0.1318,\n",
            "          -0.0301,  0.0110, -0.1772, -0.0586, -0.1212, -0.1375, -0.0932,\n",
            "          -0.3845,  0.0513, -0.0508,  0.1178,  0.0493,  0.0808, -0.0705,\n",
            "           0.1646, -0.1192,  0.1134, -0.1296,  0.2011, -0.2349,  0.1868,\n",
            "          -0.0174,  0.0236, -0.2115, -0.1556,  0.1351,  0.1111,  0.0443,\n",
            "           0.0517, -0.0278, -0.0128,  0.0045,  0.0720,  0.1068,  0.1844,\n",
            "          -0.1340, -0.1092, -0.1698,  0.0270,  0.1676,  0.1844, -0.0700,\n",
            "           0.2526, -0.0184, -0.3002, -0.0801,  0.1190, -0.0361,  0.0210,\n",
            "           0.0745,  0.1052, -0.1062, -0.1713,  0.0767,  0.1752,  0.0993,\n",
            "           0.0605, -0.0355,  0.1128, -0.2845,  0.0417,  0.0495, -0.0099,\n",
            "           0.1081,  0.1312]]], grad_fn=<StackBackward>)\n",
            "\n",
            "----------------------------------------\n",
            "必要な情報：\n",
            "out2[0] : tensor([[[-0.0330,  0.0229,  0.0605, -0.0502,  0.1159, -0.0832,  0.0181,\n",
            "          -0.0414,  0.0152,  0.0162,  0.0039,  0.0205,  0.1224, -0.0678,\n",
            "          -0.0016,  0.0030,  0.0714,  0.0776,  0.0526,  0.0383,  0.0588,\n",
            "           0.0413, -0.0026, -0.0296,  0.0914, -0.0600,  0.0766, -0.0193,\n",
            "           0.1149,  0.0862, -0.0560,  0.1501,  0.0642,  0.0824, -0.1534,\n",
            "          -0.0205, -0.0094, -0.1246,  0.0842, -0.0427,  0.0448,  0.0559,\n",
            "          -0.0647, -0.0327,  0.0112,  0.0926, -0.0233,  0.1068,  0.0093,\n",
            "          -0.0467, -0.0242,  0.0183, -0.0426, -0.0768, -0.0253, -0.0574,\n",
            "           0.0012,  0.0931,  0.0293,  0.0580,  0.0271,  0.0222, -0.0519,\n",
            "          -0.0165,  0.0062, -0.0984, -0.0308, -0.0584, -0.0633, -0.0461,\n",
            "          -0.2023,  0.0276, -0.0247,  0.0491,  0.0284,  0.0441, -0.0409,\n",
            "           0.0645, -0.0595,  0.0590, -0.0676,  0.0894, -0.1205,  0.1154,\n",
            "          -0.0084,  0.0093, -0.0984, -0.0789,  0.0636,  0.0630,  0.0213,\n",
            "           0.0259, -0.0153, -0.0056,  0.0022,  0.0338,  0.0588,  0.0948,\n",
            "          -0.0637, -0.0548, -0.0830,  0.0116,  0.0889,  0.0932, -0.0381,\n",
            "           0.1328, -0.0079, -0.1360, -0.0378,  0.0491, -0.0194,  0.0115,\n",
            "           0.0412,  0.0495, -0.0495, -0.0644,  0.0318,  0.0804,  0.0448,\n",
            "           0.0333, -0.0192,  0.0558, -0.1449,  0.0208,  0.0227, -0.0042,\n",
            "           0.0517,  0.0637]]], grad_fn=<StackBackward>)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PocitCkEpVTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn.Moduleを継承して新しいクラスを作る。決まり文句\n",
        "class LSTMClassifier(nn.Module):\n",
        "    # モデルで使う各ネットワークをコンストラクタで定義\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        # 親クラスのコンストラクタ。決まり文句\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # インプットの単語をベクトル化するために使う\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # 順伝播処理はforward関数に記載\n",
        "    def forward(self, sentence):\n",
        "        # 文章内の各単語をベクトル化して出力。2次元のテンソル\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
        "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
        "        _, lstm_out = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
        "        tag_space = self.hidden2tag(lstm_out[0].view(-1, HIDDEN_DIM))\n",
        "        # softmaxに食わせて、確率として表現\n",
        "        tag_scores = self.softmax(tag_space)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79ZXR4UL3In",
        "colab_type": "text"
      },
      "source": [
        "### 正解ラベルの変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYRtzJlLLwj3",
        "colab_type": "code",
        "outputId": "c24a4d42-1747-4130-fcb9-ed5480b5376a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# 分類先のカテゴリにIDを割り振って、そのIDの値を持つtensorを返す関数を用意\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n",
        "wk_str = 'it-life-hack'\n",
        "print('\\nwk_str : {}'.format(wk_str))\n",
        "print('category2index : {}'.format(category2index[wk_str]))\n",
        "print('category2tensor : {}'.format(category2tensor(wk_str)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dokujo-tsushin': 0, 'livedoor-homme': 1, 'kaden-channel': 2, 'smax': 3, 'topic-news': 4, 'peachy': 5, 'movie-enter': 6, 'it-life-hack': 7, 'sports-watch': 8}\n",
            "\n",
            "wk_str : it-life-hack\n",
            "category2index : 7\n",
            "category2tensor : tensor([7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta7xJ0o4MVtG",
        "colab_type": "text"
      },
      "source": [
        "### 学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v03h6SJqytA9",
        "colab_type": "code",
        "outputId": "4987e161-4a24-45f5-d4f4-4ec93bda06a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# 単語のベクトル次元数\n",
        "EMBEDDING_DIM = 10\n",
        "# 隠れ層の次元数\n",
        "HIDDEN_DIM = 128\n",
        "# データ全体の単語数\n",
        "VOCAB_SIZE = len(word2index)\n",
        "# 分類先のカテゴリの数\n",
        "TAG_SIZE = len(categories)\n",
        "# モデル宣言\n",
        "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n",
        "# 損失関数はNLLLoss()を使う。LogSoftmaxを使う時はこれを使うらしい。\n",
        "# torch.nn.CrossEntropy = torch.nn.LogSoftmax + torch.nn.NLLLoss\n",
        "loss_function = nn.NLLLoss()\n",
        "# 最適化の手法はSGDで。lossの減りに時間かかるけど、一旦はこれを使う。\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "print('device : {}'.format(device))\n",
        "pprint.pprint(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device : cuda\n",
            "LSTMClassifier(\n",
            "  (word_embeddings): Embedding(9965, 10)\n",
            "  (lstm): LSTM(10, 128)\n",
            "  (hidden2tag): Linear(in_features=128, out_features=9, bias=True)\n",
            "  (softmax): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcE6YcV18kmp",
        "colab_type": "code",
        "outputId": "eba41a4d-7aba-4a37-dc85-06eb9e05b73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 元データを7:3に分ける（7->学習、3->テスト）\n",
        "traindata, testdata = train_test_split(datasets, train_size=0.7)\n",
        "print('len(datasets) : {}'.format(len(datasets)))\n",
        "print('len(traindata) : {}'.format(len(traindata)))\n",
        "print('len(testdata) : {}'.format(len(testdata)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(datasets) : 4536\n",
            "len(traindata) : 3175\n",
            "len(testdata) : 1361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj8ezPhBMNPM",
        "colab_type": "code",
        "outputId": "95cb80bc-f90f-48e4-a069-f4fa80211d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print ('training start ...')\n",
        "\n",
        "# 各エポックの合計loss値を格納する\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# ループ回してみる。（バッチ化とか使ってないので結構時間かかる...）\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # initialize each epoch\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "\n",
        "    # ネットワークを学習モードに切り替える\n",
        "    model.train()\n",
        "    for title, cat in zip(traindata[\"title\"], traindata[\"category\"]):\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        # 毎回バックプロパゲーションの初期値をリセット(勾配リセット)\n",
        "        # model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        # 文章を単語IDの系列に変換（modelに食わせられる形に変換）[２次元]\n",
        "        # LSTMモデル内では、２次元→３次元をしている\n",
        "        # (例) tensor([ 472, 15, 1783, 285, 1207, 37, 13, 56, 1012, 1, 689, 113])\n",
        "        inputs = sentence2index(title).to(device)\n",
        "        # 順方向の伝播の計算(forwardが呼ばれる)\n",
        "        outputs = model(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        # (例) tensor([7])\n",
        "        answer = category2tensor(cat).to(device)\n",
        "        # 損失関数：予測値と正解ラベル値の違いを計算\n",
        "        loss = loss_function(outputs, answer)\n",
        "        # 誤差のバックプロパゲーション(逆伝播)で勾配を求める\n",
        "        loss.backward()\n",
        "        # バックプロパゲーションで計算した勾配を元に重みを更新\n",
        "        optimizer.step()\n",
        "        # lossを集計\n",
        "        train_loss += loss.item()\n",
        "        # 9パターンの分類毎に値が取得できるので、最も値が高い分類をmax(1)で取得\n",
        "        # max(1)の戻り値の[0]は精度(0〜1)、[1]は予測ラベル\n",
        "        # sum()で、予測と正解ラベルが合っている数をTensor形式でカウント\n",
        "        # item()で、Tensorをintへ\n",
        "        acc = (outputs.max(1)[1] == answer).sum()\n",
        "        train_acc += acc.item()\n",
        "\n",
        "    # lossとaccの平均を計算\n",
        "    avg_train_loss = train_loss / len(traindata)\n",
        "    avg_train_acc = train_acc / len(traindata)\n",
        "\n",
        "    # ネットワークを推論モードに切り替える\n",
        "    model.eval()\n",
        "    # torch.no_gradで必要のない計算を停止 パラメータの保存を止める(test時にメモリが溢れてしまうのを防止)\n",
        "    with torch.no_grad():\n",
        "        for title, cat in zip(testdata[\"title\"], testdata[\"category\"]):\n",
        "            optimizer.zero_grad()\n",
        "            inputs = sentence2index(title).to(device)\n",
        "            outputs = model(inputs)\n",
        "            answer = category2tensor(cat).to(device)\n",
        "            #loss = loss_function(outputs, answer)\n",
        "            #loss.backward()\n",
        "            #optimizer.step()\n",
        "            #test_loss += loss.item()\n",
        "            acc = (outputs.max(1)[1] == answer).sum()\n",
        "            test_acc += acc.item()\n",
        "        avg_test_loss = test_loss / len(testdata)\n",
        "        avg_test_acc = test_acc / len(testdata)\n",
        "\n",
        "    print(\"Epoch {}/{} \\t train_loss : {:.5f} \\t train_acc : {:.5f} \\t test_loss : {:.5f} \\t test_acc : {:.5f}\"\n",
        "                                    .format(epoch+1, num_epochs, avg_train_loss, avg_train_acc, avg_test_loss, avg_test_acc))\n",
        "\n",
        "    # append list for polt graph after training\n",
        "    train_loss_list.append(avg_train_loss)\n",
        "    train_acc_list.append(avg_train_acc)\n",
        "    test_loss_list.append(avg_test_loss)\n",
        "    test_acc_list.append(avg_test_acc)\n",
        "\n",
        "print(\"training end ...\")\n",
        "\n",
        "# save model\n",
        "with open(BASE_PATH + 'model/ldcc_lstm_model.pkl', 'wb') as f:\n",
        "    cloudpickle.dump(model, f)\n",
        "print(\"saved training model ...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start ...\n",
            "Epoch 1/100 \t train_loss : 1.90967 \t train_acc : 0.26740 \t test_loss : 0.00000 \t test_acc : 0.26672\n",
            "Epoch 2/100 \t train_loss : 1.74505 \t train_acc : 0.34614 \t test_loss : 0.00000 \t test_acc : 0.35856\n",
            "Epoch 3/100 \t train_loss : 1.60782 \t train_acc : 0.41921 \t test_loss : 0.00000 \t test_acc : 0.42101\n",
            "Epoch 4/100 \t train_loss : 1.49098 \t train_acc : 0.48189 \t test_loss : 0.00000 \t test_acc : 0.49155\n",
            "Epoch 5/100 \t train_loss : 1.39551 \t train_acc : 0.52283 \t test_loss : 0.00000 \t test_acc : 0.53417\n",
            "Epoch 6/100 \t train_loss : 1.31568 \t train_acc : 0.55087 \t test_loss : 0.00000 \t test_acc : 0.55253\n",
            "Epoch 7/100 \t train_loss : 1.24410 \t train_acc : 0.57858 \t test_loss : 0.00000 \t test_acc : 0.57678\n",
            "Epoch 8/100 \t train_loss : 1.17705 \t train_acc : 0.60661 \t test_loss : 0.00000 \t test_acc : 0.58486\n",
            "Epoch 9/100 \t train_loss : 1.11349 \t train_acc : 0.63118 \t test_loss : 0.00000 \t test_acc : 0.60911\n",
            "Epoch 10/100 \t train_loss : 1.05342 \t train_acc : 0.65543 \t test_loss : 0.00000 \t test_acc : 0.62307\n",
            "Epoch 11/100 \t train_loss : 0.99774 \t train_acc : 0.67780 \t test_loss : 0.00000 \t test_acc : 0.63483\n",
            "Epoch 12/100 \t train_loss : 0.94034 \t train_acc : 0.69701 \t test_loss : 0.00000 \t test_acc : 0.64291\n",
            "Epoch 13/100 \t train_loss : 0.88725 \t train_acc : 0.70551 \t test_loss : 0.00000 \t test_acc : 0.64585\n",
            "Epoch 14/100 \t train_loss : 0.83434 \t train_acc : 0.72504 \t test_loss : 0.00000 \t test_acc : 0.65026\n",
            "Epoch 15/100 \t train_loss : 0.77891 \t train_acc : 0.74425 \t test_loss : 0.00000 \t test_acc : 0.64805\n",
            "Epoch 16/100 \t train_loss : 0.72050 \t train_acc : 0.76693 \t test_loss : 0.00000 \t test_acc : 0.64732\n",
            "Epoch 17/100 \t train_loss : 0.65995 \t train_acc : 0.78551 \t test_loss : 0.00000 \t test_acc : 0.65907\n",
            "Epoch 18/100 \t train_loss : 0.60707 \t train_acc : 0.80409 \t test_loss : 0.00000 \t test_acc : 0.66128\n",
            "Epoch 19/100 \t train_loss : 0.52943 \t train_acc : 0.83024 \t test_loss : 0.00000 \t test_acc : 0.66201\n",
            "Epoch 20/100 \t train_loss : 0.46226 \t train_acc : 0.85386 \t test_loss : 0.00000 \t test_acc : 0.66569\n",
            "Epoch 21/100 \t train_loss : 0.39330 \t train_acc : 0.87591 \t test_loss : 0.00000 \t test_acc : 0.66495\n",
            "Epoch 22/100 \t train_loss : 0.35400 \t train_acc : 0.89008 \t test_loss : 0.00000 \t test_acc : 0.66863\n",
            "Epoch 23/100 \t train_loss : 0.30255 \t train_acc : 0.90614 \t test_loss : 0.00000 \t test_acc : 0.67083\n",
            "Epoch 24/100 \t train_loss : 0.23351 \t train_acc : 0.93732 \t test_loss : 0.00000 \t test_acc : 0.66863\n",
            "Epoch 25/100 \t train_loss : 0.20610 \t train_acc : 0.94299 \t test_loss : 0.00000 \t test_acc : 0.67303\n",
            "Epoch 26/100 \t train_loss : 0.14750 \t train_acc : 0.96850 \t test_loss : 0.00000 \t test_acc : 0.66863\n",
            "Epoch 27/100 \t train_loss : 0.10383 \t train_acc : 0.98268 \t test_loss : 0.00000 \t test_acc : 0.67450\n",
            "Epoch 28/100 \t train_loss : 0.07909 \t train_acc : 0.98929 \t test_loss : 0.00000 \t test_acc : 0.67010\n",
            "Epoch 29/100 \t train_loss : 0.06211 \t train_acc : 0.99276 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 30/100 \t train_loss : 0.04732 \t train_acc : 0.99654 \t test_loss : 0.00000 \t test_acc : 0.66642\n",
            "Epoch 31/100 \t train_loss : 0.03800 \t train_acc : 0.99685 \t test_loss : 0.00000 \t test_acc : 0.66863\n",
            "Epoch 32/100 \t train_loss : 0.03142 \t train_acc : 0.99780 \t test_loss : 0.00000 \t test_acc : 0.66275\n",
            "Epoch 33/100 \t train_loss : 0.02454 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66128\n",
            "Epoch 34/100 \t train_loss : 0.02128 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66275\n",
            "Epoch 35/100 \t train_loss : 0.01777 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66201\n",
            "Epoch 36/100 \t train_loss : 0.01565 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66201\n",
            "Epoch 37/100 \t train_loss : 0.01398 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66128\n",
            "Epoch 38/100 \t train_loss : 0.01267 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 39/100 \t train_loss : 0.01156 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.65981\n",
            "Epoch 40/100 \t train_loss : 0.01068 \t train_acc : 0.99811 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 41/100 \t train_loss : 0.00986 \t train_acc : 0.99843 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 42/100 \t train_loss : 0.00925 \t train_acc : 0.99843 \t test_loss : 0.00000 \t test_acc : 0.66201\n",
            "Epoch 43/100 \t train_loss : 0.00878 \t train_acc : 0.99843 \t test_loss : 0.00000 \t test_acc : 0.65907\n",
            "Epoch 44/100 \t train_loss : 0.00824 \t train_acc : 0.99843 \t test_loss : 0.00000 \t test_acc : 0.65834\n",
            "Epoch 45/100 \t train_loss : 0.00762 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 46/100 \t train_loss : 0.00711 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 47/100 \t train_loss : 0.00678 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66128\n",
            "Epoch 48/100 \t train_loss : 0.00650 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66128\n",
            "Epoch 49/100 \t train_loss : 0.00625 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 50/100 \t train_loss : 0.00603 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65981\n",
            "Epoch 51/100 \t train_loss : 0.00583 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65907\n",
            "Epoch 52/100 \t train_loss : 0.00565 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 53/100 \t train_loss : 0.00548 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.66054\n",
            "Epoch 54/100 \t train_loss : 0.00533 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65981\n",
            "Epoch 55/100 \t train_loss : 0.00519 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65760\n",
            "Epoch 56/100 \t train_loss : 0.00507 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65760\n",
            "Epoch 57/100 \t train_loss : 0.00495 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65760\n",
            "Epoch 58/100 \t train_loss : 0.00484 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65760\n",
            "Epoch 59/100 \t train_loss : 0.00474 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65760\n",
            "Epoch 60/100 \t train_loss : 0.00464 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65687\n",
            "Epoch 61/100 \t train_loss : 0.00456 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65687\n",
            "Epoch 62/100 \t train_loss : 0.00447 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 63/100 \t train_loss : 0.00440 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 64/100 \t train_loss : 0.00432 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65540\n",
            "Epoch 65/100 \t train_loss : 0.00425 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 66/100 \t train_loss : 0.00419 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 67/100 \t train_loss : 0.00413 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 68/100 \t train_loss : 0.00407 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65540\n",
            "Epoch 69/100 \t train_loss : 0.00401 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 70/100 \t train_loss : 0.00396 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65687\n",
            "Epoch 71/100 \t train_loss : 0.00391 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65687\n",
            "Epoch 72/100 \t train_loss : 0.00387 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 73/100 \t train_loss : 0.00382 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 74/100 \t train_loss : 0.00378 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 75/100 \t train_loss : 0.00374 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65540\n",
            "Epoch 76/100 \t train_loss : 0.00370 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65540\n",
            "Epoch 77/100 \t train_loss : 0.00366 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65540\n",
            "Epoch 78/100 \t train_loss : 0.00363 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 79/100 \t train_loss : 0.00359 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 80/100 \t train_loss : 0.00356 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 81/100 \t train_loss : 0.00353 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 82/100 \t train_loss : 0.00350 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 83/100 \t train_loss : 0.00347 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 84/100 \t train_loss : 0.00344 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65614\n",
            "Epoch 85/100 \t train_loss : 0.00341 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65467\n",
            "Epoch 86/100 \t train_loss : 0.00339 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65467\n",
            "Epoch 87/100 \t train_loss : 0.00336 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65467\n",
            "Epoch 88/100 \t train_loss : 0.00334 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65467\n",
            "Epoch 89/100 \t train_loss : 0.00332 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65467\n",
            "Epoch 90/100 \t train_loss : 0.00329 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65467\n",
            "Epoch 91/100 \t train_loss : 0.00327 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65320\n",
            "Epoch 92/100 \t train_loss : 0.00325 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65320\n",
            "Epoch 93/100 \t train_loss : 0.00323 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65246\n",
            "Epoch 94/100 \t train_loss : 0.00321 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65246\n",
            "Epoch 95/100 \t train_loss : 0.00319 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65246\n",
            "Epoch 96/100 \t train_loss : 0.00317 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65246\n",
            "Epoch 97/100 \t train_loss : 0.00315 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65320\n",
            "Epoch 98/100 \t train_loss : 0.00314 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65246\n",
            "Epoch 99/100 \t train_loss : 0.00312 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65246\n",
            "Epoch 100/100 \t train_loss : 0.00310 \t train_acc : 0.99874 \t test_loss : 0.00000 \t test_acc : 0.65173\n",
            "training end ...\n",
            "saved training model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExxI2cEduxLF",
        "colab_type": "text"
      },
      "source": [
        "#### 学習結果をグラフで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkzVtrMoutKo",
        "colab_type": "code",
        "outputId": "3ea439f6-c6c7-4d2c-c3a9-448191b96b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "#plt.plot(train_loss_list)\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(num_epochs), train_loss_list, color='blue', linestyle='-', label='train_loss')\n",
        "plt.plot(range(num_epochs), test_loss_list, color='green', linestyle='--', label='test_loss')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(num_epochs), train_acc_list, color='blue', linestyle='-', label='train_acc')\n",
        "plt.plot(range(num_epochs), test_acc_list, color='green', linestyle='--', label='test_acc')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.title('Training and test acc')\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFNCAYAAABfWL0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREUpwaPxERERERkRT3/6mX1wLoRuX1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}